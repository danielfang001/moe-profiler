================================================================================
OLMoE-1B-7B ARCHITECTURE RESEARCH - EXECUTIVE SUMMARY
================================================================================

RESEARCH COMPLETION: November 13, 2025

Your questions and direct answers:

1. EXACT MODULE NAME FOR MoE ROUTER/GATE THAT DOES EXPERT SELECTION
   
   Answer: OlmoeTopKRouter
   Location: model.layers[i].mlp.gate
   Full Path: transformers.models.olmoe.modeling_olmoe.OlmoeTopKRouter

2. MODULE STRUCTURE FOR MoE LAYERS
   
   Answer: OlmoeSparseMoeBlock
   Location: model.layers[i].mlp
   Structure:
   - .gate → OlmoeTopKRouter (routes tokens to experts)
   - .experts → OlmoeExperts (expert FFN pool)

3. HuggingFace/TRANSFORMERS IMPLEMENTATION
   
   File: src/transformers/models/olmoe/modeling_olmoe.py
   All classes defined in transformers library

4. MODULE THAT RETURNS (ROUTING_WEIGHTS, EXPERT_INDICES)
   
   Answer: OlmoeTopKRouter.forward()
   Returns: (router_scores, router_indices) tuple
   Shapes:
   - router_scores: [num_tokens, num_experts] (softmax-normalized)
   - router_indices: [num_tokens, k=8] (selected expert indices)

5. NAMING CONVENTION USED
   
   Answer: 'gate' (not 'router')
   Found in: self.gate = OlmoeTopKRouter(config)

================================================================================
ARCHITECTURE OVERVIEW
================================================================================

Model Configuration:
- Model Type: Sparse Mixture-of-Experts (MoE)
- Active Parameters: 1B
- Total Parameters: 7B
- Layers: 16 transformer layers
- Hidden Dimension: 2048
- Experts Per Layer: 64 total experts
- Experts Per Token: 8 (top-k selection)
- Routing Algorithm: Dropless token-based with softmax + top-k

Complete Naming Hierarchy:
```
model.layers[i]                    → OlmoeDecoderLayer
  .self_attn                       → OlmoeAttention
  .mlp                             → OlmoeSparseMoeBlock (MoE container)
    .gate                          → OlmoeTopKRouter (ROUTER)
    .experts                       → OlmoeExperts (Expert Pool)
  .input_layernorm                 → OlmoeRMSNorm
  .post_attention_layernorm        → OlmoeRMSNorm
```

================================================================================
ROUTER IMPLEMENTATION DETAILS
================================================================================

OlmoeTopKRouter Class:
- Parameter: weight [num_experts=64, hidden_dim=2048]
- Forward Process:
  1. Linear projection: logits = F.linear(hidden_states, weight)
  2. Softmax: probs = softmax(logits)
  3. Top-k selection: top_values, indices = topk(probs, k=8)
  4. Optional renormalization
  5. Scatter to create sparse matrix
  6. Return: (router_scores, router_indices)

Output Format:
- router_scores: Dense matrix with softmax weights at top-k positions
- router_indices: Integer indices of selected experts per token
- Both tensors support gradient flow for training

================================================================================
INTEGRATION WITH MOE-PROFILER
================================================================================

The moe-profiler's SimpleRouterWrapper automatically handles OLMoE:

Key Detection:
- Searches for 'gate' or 'router' in module names
- Wraps all OlmoeTopKRouter instances
- Supports the (scores, indices) tuple output format

Automatic Detection:
if isinstance(router_output, tuple) and len(router_output) == 2:
    routing_weights, expert_indices = router_output

Metrics Collected Per Router:
- FLOPs (total, router, expert)
- Latency (ms precision)
- Expert loads (imbalance metrics)
- Router confidence (entropy-based)
- Active experts per forward pass
- K distribution (dynamic expert selection)

================================================================================
KEY FILES CREATED
================================================================================

1. OLMOE_ARCHITECTURE.md
   - Complete class definitions
   - Detailed code implementation
   - Configuration parameters
   - Architecture diagrams

2. OLMOE_PROFILING_GUIDE.md
   - Quick start examples
   - Metrics explanation
   - Troubleshooting guide
   - Advanced usage patterns

3. MOE_MODEL_COMPARISON.md
   - OLMoE vs Mixtral comparison
   - Router algorithm details
   - Parameter inspection guide
   - Summary tables

================================================================================
QUICK REFERENCE
================================================================================

Access the router:
```python
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained("allenai/OLMoE-1B-7B-0924-Instruct")
router = model.layers[0].mlp.gate  # OlmoeTopKRouter instance
```

Get router output:
```python
router_scores, router_indices = router(hidden_states)
# router_scores: [num_tokens, 64] softmax weights
# router_indices: [num_tokens, 8] selected expert IDs
```

Profile with moe-profiler:
```python
from profiler import MoEProfiler
profiler = MoEProfiler(model)
profiler.start()
# ... run model ...
profiler.print_summary()
```

================================================================================
CONFIGURATION PARAMETERS
================================================================================

Important OlmoeConfig values:
- num_experts: 64 (routed experts per layer)
- num_experts_per_tok: 8 (experts selected per token)
- hidden_size: 2048
- intermediate_size: 2048
- num_hidden_layers: 16
- num_attention_heads: 16
- hidden_act: "silu"
- router_aux_loss_coef: 0.01
- norm_topk_prob: True (renormalize top-k)

================================================================================
TRANSFORMER LIBRARY LOCATION
================================================================================

Source Code: github.com/huggingface/transformers
Directory: src/transformers/models/olmoe/

Key Files:
- modeling_olmoe.py → Class definitions
- configuration_olmoe.py → Config parameters

Classes in modeling_olmoe.py:
- OlmoeTopKRouter → Router/Gate
- OlmoeExperts → Expert pool
- OlmoeSparseMoeBlock → MoE container
- OlmoeDecoderLayer → Full transformer layer
- OlmoeModel → Main model
- OlmoeForCausalLM → Causal LM version

================================================================================
RESEARCH COMPLETION STATUS
================================================================================

All requested information has been researched and documented:

[✓] Exact module name for MoE router/gate
[✓] Module structure for MoE layers
[✓] HuggingFace/transformers library code location
[✓] Module that returns (routing_weights, expert_indices)
[✓] Naming convention used (gate vs router)
[✓] Complete class definitions
[✓] Configuration parameters
[✓] Integration guide for moe-profiler
[✓] Comparison with other MoE models

================================================================================
For questions or implementation help, refer to:
- OLMOE_ARCHITECTURE.md (detailed specifications)
- OLMOE_PROFILING_GUIDE.md (practical usage)
- MOE_MODEL_COMPARISON.md (comparison with other models)
================================================================================
